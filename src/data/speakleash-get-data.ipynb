{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"news_14_general_corpus\",\n",
    "\"news_13_general_corpus\",\n",
    "\"news_8_general_corpus\",\n",
    "\"news_30_general_corpus\",\n",
    "\"news_22_general_corpus\",\n",
    "\"news_15_general_corpus\",\n",
    "\"news_18_general_corpus\",\n",
    "\"news_27_general_corpus\",\n",
    "\"news_20_general_corpus\",\n",
    "\"news_21_it_corpus\",\n",
    "\"news_16_general_corpus\",\n",
    "\"news_24_general_corpus\",\n",
    "\"news_19_general_corpus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_14_general_corpus\n",
      "news_13_general_corpus\n",
      "news_8_general_corpus\n",
      "news_30_general_corpus\n",
      "news_22_general_corpus\n",
      "news_15_general_corpus\n",
      "news_18_general_corpus\n",
      "news_27_general_corpus\n",
      "news_20_general_corpus\n",
      "news_21_it_corpus\n",
      "news_16_general_corpus\n",
      "news_24_general_corpus\n",
      "news_19_general_corpus\n"
     ]
    }
   ],
   "source": [
    "from speakleash import Speakleash\n",
    "import os\n",
    "\n",
    "replicate_to = os.path.join(\"../../data/polish-pretrain/\")\n",
    "\n",
    "sl = Speakleash(replicate_to)\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    wiki = sl.get(dataset).data\n",
    "    \n",
    "    # for doc in wiki:\n",
    "    #     print(doc[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unfinished_last_sentence(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # Check if the last sentence is unfinished (no period at the end)\n",
    "    if not sentences[-1].endswith('.'):\n",
    "        # Remove the last sentence\n",
    "        sentences = sentences[:-1]\n",
    "    \n",
    "    # Join the sentences back into text\n",
    "    return '.'.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Auto. Tragaa'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_unfinished_last_sentence(\"Auto. Tragaa. okkk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    wiki = sl.get(dataset).data    \n",
    "    for doc in wiki:\n",
    "        x = remove_unfinished_last_sentence(doc[:-8192])\n",
    "        words.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [s for s in words if s != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(words[2], return_tensors=\"pt\", return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(words)\n",
    "hf_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(words)\n",
    "df = df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "df_train = df.iloc[2000:]\n",
    "df_train.columns = [\"text\"]\n",
    "df_test = df.iloc[:2000]\n",
    "df_test.columns = [\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DatasetDict()\n",
    "ds[\"train\"] = Dataset.from_pandas(df_train)\n",
    "ds[\"validation\"] = Dataset.from_pandas(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 595540\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19e5649ff5a4f52b619868a219b27e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03d9ce5813e46ae9514914d8ac64d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fc5b79c4c4425e97467ecace9a484c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7a51094e48447da09a87f83165be3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb59213271e843b78cf9a9f1773ffbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac14ebf03904954812cf321f2cc9dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19e8047f20b405aacd555726bcef5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e96f4d2eb24417b9ad2b3705043642a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/szymonrucinski/krakowiak-pretrain-pl/commit/efe02a8d3274fd64d5ef8e0504a5dd79013dd63a', commit_message='Upload dataset', commit_description='', oid='efe02a8d3274fd64d5ef8e0504a5dd79013dd63a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(\"szymonrucinski/krakowiak-pretrain-pl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "krakowiak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
